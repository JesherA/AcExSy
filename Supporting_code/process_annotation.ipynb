{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a06ecc0e",
   "metadata": {},
   "source": [
    "# User annotation pre-processing script\n",
    "\n",
    "This script can be used to manualy clean the user annotations for the AcX system.\n",
    "\n",
    "### Preparation:\n",
    " - Use google-cloud-sdk to download the annotations in .json format\n",
    " - Add the .json files to single folder (I called it 'google_annotation/json_data/', but you can of course change the name and code\n",
    " - When it is the first time you clean the data, commend the following lines:\n",
    "     - clean_annotations = pd.read_csv('./clean_data/clean_annotations_nl.csv')\n",
    "     - already_clean_docs = set(clean_annotations['doc_id'])\n",
    "     - !! ALSO, REMOVE \"already_clean_docs\" from dirty_docs = total_docs - already_clean_docs !!\n",
    "     - undo all of this once you've cleaned up the first documents and have a clean_annotations_XX.csv file.\n",
    "\n",
    "### Cleaning the documents:\n",
    "* [Difference Finder](#Difference-Finder): The code in these cells checks the differences between the annotators of the same document. There are two common differences: \n",
    "     1. One annotator missed an acronym\n",
    "     2. The annotators wrote the acronym or expansion in two different ways.\n",
    "A document gets returned if one or both of these differences occurs. \n",
    "\n",
    "* [Manual cleaning](#Manual-cleaning)\n",
    "Cleaning the documents is done using the pandas' framework. Please follow the following steps:\n",
    "    1. Enter the document of intrest --> document = '....'\n",
    "    2. Check the output of differences and add the row numbers in the cell below (row_num_1 & row_num_2)\n",
    "    3. Use the next cell to change the values inside the data frame. You can change values by using the .iloc[.., ..] statement or add a row with the extra_row variable in combination with the append statement. \n",
    "    4. All the acronyms and expansions for the document of interest are printed below for one last check.\n",
    "    5. [Saving changes](#Saving-changes) Finally, You can append and save the output to the clean_annotations_xx.csv. I would recommend doing this after every document. \n",
    "\n",
    "### Some other issues you might encounter \n",
    "- Mail issue: The document IDs were generated by splitting the names of the .json files based on a _ . However, some emails use an underscore, which will create improper document splits. This issue is solved by explicitly splitting on the name. Therefore, add all email addresses with an underscore to the \"exception_mails\" list.\n",
    "- Documents with only one annotator: Some documents will not yet be annotated by two people. You can ignore these documents for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for processing and cleaning the google drive annotations.\n",
    "The output is a .csv file with the acronyms and expansions per document.\n",
    "Date: 23-05-2022\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6eddcc",
   "metadata": {},
   "source": [
    "## Loading the Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4879ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/Users/jesher/Desktop/Master data science UvA/Semester 2/Thesis/google_annotation/json_data/'\n",
    "df = pd.DataFrame(columns=['acronym', 'expansion', 'language', 'type'])\n",
    "\n",
    "# Loading the clean annotations (Only you run this if you have clean annotations already)\n",
    "clean_annotations = pd.read_csv('./clean_data/clean_annotations_nl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_raw_annotation(df, rootdir):\n",
    "    # Creating the file directories\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            abs_path = os.path.join(subdir, file)\n",
    "            individual_json = pd.read_json(abs_path)\n",
    "\n",
    "            # Extracting the annotators\n",
    "            exception_mails = ['jesher_a@hotmail.com']\n",
    "\n",
    "            # filer for mails with an underscore\n",
    "            for i in exception_mails:\n",
    "                if bool(re.search(i, abs_path.split('/')[-1])):\n",
    "                    annotators = i\n",
    "                else:\n",
    "                    annotators = abs_path.split('_')[-1]\n",
    "                    annotators = annotators[:-5]\n",
    "\n",
    "            # Extracting the document ID's\n",
    "            doc_id = abs_path.split('/')[-1]\n",
    "            doc_id = doc_id.replace(annotators, '')[:-6]\n",
    "\n",
    "            # Transforming the json  in a proper format\n",
    "            individual_json = individual_json.transpose()\n",
    "            individual_json.reset_index(inplace=True)\n",
    "            individual_json = individual_json.rename(columns={'index':'acronym'})\n",
    "            individual_json['doc_id'] = doc_id\n",
    "            individual_json['annotator'] = annotators\n",
    "\n",
    "            # adding everything together in a df\n",
    "            df = pd.concat([df, individual_json], ignore_index=True)\n",
    "\n",
    "            # fill the missing values in the language column\n",
    "            df['language'].replace('', \"other language\", inplace=True)\n",
    "            \n",
    "            # remove additional white spaces\n",
    "            df['acronym'] = df['acronym'].str.strip()\n",
    "            df['expansion'] = df['expansion'].str.strip()\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e6d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only at the beginning !!!!!!! All clean annotations will be lost if executed \n",
    "annotation_df = cleaning_raw_annotation(df, rootdir)\n",
    "annotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus info\n",
    "print('Number of annotators: {}'.format(len(set(annotation_df['annotator']))))\n",
    "print('Number of documents: {}'.format(len(set(annotation_df['doc_id']))))\n",
    "print('Number of clean documents: {}'.format(len(set(clean_annotations['doc_id']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4e143",
   "metadata": {},
   "source": [
    "## Difference Finder <a class=\"anchor\" id=\"Difference-Finder\"></a>\n",
    "The following code section looks for the differences between the annotated documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad509608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "total_docs =set(annotation_df['doc_id'])\n",
    "already_clean_docs = set(clean_annotations['doc_id'])\n",
    "\n",
    "# dirty document info\n",
    "dirty_docs = total_docs - already_clean_docs\n",
    "print(\"Number of dirty documents:{}\".format(len(dirty_docs)))\n",
    "\n",
    "\n",
    "# Return all doc_id's with an uneven number of acroyms (which means...)\n",
    "print(\"\\nDOCUMENTS WITH AN UNEVEN NUMBER OF ACRONYMS:\")\n",
    "dirt_1 = []\n",
    "for i in set(annotation_df['doc_id']):\n",
    "    if i in dirty_docs:\n",
    "        if len(annotation_df[annotation_df['doc_id'] == str(i)]) % 2 != 0:\n",
    "            dirt_1.append(i)\n",
    "            print(\"  -\", i)\n",
    "        \n",
    "print(\"\\nDOCUMENTS WITH DUPLICATE ROWS:\")\n",
    "dirt_2 = []\n",
    "for i in set(annotation_df['doc_id']):\n",
    "    sub_set_df = annotation_df[annotation_df['doc_id'] == i]\n",
    "    if i in dirty_docs:\n",
    "    # Delete duplicate rows\n",
    "        if len(sub_set_df.drop_duplicates(subset=[\"acronym\", \"expansion\"], keep=False)) != 0:\n",
    "            dirt_2.append(i)\n",
    "            print(\"  -\", i)\n",
    "\n",
    "del sub_set_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4867e2",
   "metadata": {},
   "source": [
    "## Adding clean documents to the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d601afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dirty =  dirt_2 + dirt_1\n",
    "doc_subs = set.union(already_clean_docs, all_dirty)\n",
    "\n",
    "\n",
    "all_docs = set(annotation_df['doc_id'])\n",
    "good_docs = all_docs - doc_subs\n",
    "print('Clean documents: \\n{}'.format(good_docs))\n",
    "\n",
    "\n",
    "\n",
    "# adding all clean documents together\n",
    "clean_doc_all = pd.DataFrame(columns=['acronym', 'expansion','language', 'type', 'doc_id','annotator'])\n",
    "\n",
    "for i in good_docs:\n",
    "    df_empty = pd.DataFrame(columns=['acronym', 'expansion','language', 'type', 'doc_id','annotator'])\n",
    "    sub_df = annotation_df[annotation_df['doc_id'] == i]\n",
    "    clean_doc = pd.concat([df_empty, sub_df])\n",
    "    clean_doc_all = pd.concat([clean_doc_all, clean_doc])\n",
    "    \n",
    "if set(clean_doc_all['doc_id']) == good_docs:\n",
    "    print('\\nAll good')\n",
    "    \n",
    "\n",
    "clean_annotations = pd.concat([clean_annotations, clean_doc_all])\n",
    "\n",
    "# Save the changes in the main .csv file\n",
    "# clean_annotations.to_csv('./clean_data/clean_annotations_nl.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c695b013",
   "metadata": {},
   "source": [
    "## Manual cleaning<a class=\"anchor\" id=\"Manual-cleaning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8890a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annotation_df.iloc[row_num_1, 1])\n",
    "print(annotation_df.iloc[row_num_2, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the annotations that have issues\n",
    "document = \"Maybach\"\n",
    "\n",
    "sub_set_df = annotation_df[annotation_df['doc_id'] == document]\n",
    "sub_set_df.drop_duplicates(subset=[\"acronym\", \"expansion\"], keep=False).sort_values(by='acronym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d30dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the index to see of the values are the same\n",
    "row_num_1 = 375\n",
    "row_num_2 = 998\n",
    "\n",
    "# Results\n",
    "print(\"Are the acronyms the same:\")\n",
    "print(annotation_df.iloc[row_num_1, 0] == annotation_df.iloc[row_num_2, 0])\n",
    "print(\"\\nAre the expansion the same:\")\n",
    "print(annotation_df.iloc[row_num_1, 1] == annotation_df.iloc[row_num_2, 1])\n",
    "print(annotation_df.iloc[row_num_1, 1])\n",
    "print(annotation_df.iloc[row_num_2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d442f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "# fill in the index of the cell you want to change\n",
    "# annotation_df.iloc[80, 0] = \"MD\"\n",
    "\n",
    "# Use the code below if you need toadd a new row\n",
    "# extra_row = {'acronym':'ITV-netwerk', 'expansion':'Independent Television-netwerk', 'language':'en', 'type':'out_expansion', 'doc_id':document, 'annotator':'jesher.appels@gmail.com'}\n",
    "\n",
    "\n",
    "# Show the results\n",
    "# annotation_df = annotation_df.append(extra_row, ignore_index = True)         # <-- uncomment if you need to add a extra row\n",
    "results = annotation_df[annotation_df['doc_id'] == document]  \n",
    "results.sort_values(by='acronym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708953f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.drop(565, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0378e",
   "metadata": {},
   "source": [
    "## Saving the changes<a class=\"anchor\" id=\"\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the changes\n",
    "df_empty = pd.DataFrame(columns=['acronym', 'expansion','language', 'type', 'doc_id','annotator'])\n",
    "clean_annotations_sub = pd.concat([df_empty, results])\n",
    "clean_annotations = pd.concat([clean_annotations, clean_annotations_sub])\n",
    "\n",
    "# Save the changes in the main .csv file\n",
    "clean_annotations.to_csv('./clean_data/clean_annotations_nl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cbddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(clean_annotations['doc_id'].sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51072c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# clean_annotations[clean_annotations['doc_id'] =='Waterschapsverkiezingen'].sort_values(by='acronym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f0cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_annotations.drop_duplicates(keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c1b79",
   "metadata": {},
   "source": [
    "## Adding a seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name  = \"/Users/jesher/Desktop/Master data science UvA/Semester 2/Thesis/google_annotation/clean_data/clean_annotations_nl_wouter.csv\"\n",
    "# second_file = pd.read_csv(file_name)\n",
    "\n",
    "# second_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_annotations = pd.concat([clean_annotations, second_file])\n",
    "\n",
    "# Save the changes in the main .csv file\n",
    "# clean_annotations.to_csv('./clean_data/clean_annotations_nl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916c612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
