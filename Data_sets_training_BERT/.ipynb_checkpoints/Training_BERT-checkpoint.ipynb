{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae8d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning BERT with sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0608e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa428a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8cfe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n_/qtb6cclx4hg7tt04lvl1_m400000gn/T/ipykernel_1346/170282124.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_large = pd.read_csv('SDU-AAAI-AD_sentence_pairs_BERT.csv')\n"
     ]
    }
   ],
   "source": [
    "# loading the data SDU data set with sentence pairs \n",
    "data_large = pd.read_csv('SDU-AAAI-AD_sentence_pairs_BERT.csv')\n",
    "\n",
    "# creating a subset of the data set\n",
    "data = data_large.head(150)\n",
    "del data_large\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c0f2cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>acronym</th>\n",
       "      <th>expansion</th>\n",
       "      <th>tokens_x</th>\n",
       "      <th>short_form_x</th>\n",
       "      <th>tokens_y</th>\n",
       "      <th>short_form_y</th>\n",
       "      <th>pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12863349</td>\n",
       "      <td>19</td>\n",
       "      <td>deep belief network</td>\n",
       "      <td>['If', 'we', 'compare', 'the', 'computational'...</td>\n",
       "      <td>DBN</td>\n",
       "      <td>['Moreover', ',', 'it', 'has', 'not', 'been', ...</td>\n",
       "      <td>DBN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15091181</td>\n",
       "      <td>37</td>\n",
       "      <td>strictly local</td>\n",
       "      <td>['Furthermore', ',', 'since', 'the', 'LSTMs', ...</td>\n",
       "      <td>SL</td>\n",
       "      <td>['Consequently', ',', 'for', 'any', 'SL', '(',...</td>\n",
       "      <td>SL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2157740</td>\n",
       "      <td>2</td>\n",
       "      <td>convolutional neural network</td>\n",
       "      <td>['ResultsResults', 'for', 'CNN', 'trained', 'o...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>['These', 'baseline', 'models', 'are', 'select...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>13210757</td>\n",
       "      <td>4</td>\n",
       "      <td>augmented reality</td>\n",
       "      <td>['d', ')', ':', 'LayAR', 'AR', 'browser', '.',...</td>\n",
       "      <td>AR</td>\n",
       "      <td>['At', 'this', 'stage', 'the', 'system', 'was'...</td>\n",
       "      <td>AR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1839840</td>\n",
       "      <td>6</td>\n",
       "      <td>convolutional neural network</td>\n",
       "      <td>['This', 'paper', 'demonstrates', 'the', 'effe...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>['For', 'every', 'testing', 'image', '(', 'the...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     index  acronym                     expansion  \\\n",
       "0           0  12863349       19           deep belief network   \n",
       "1           1  15091181       37                strictly local   \n",
       "2           2   2157740        2  convolutional neural network   \n",
       "3           3  13210757        4             augmented reality   \n",
       "4           4   1839840        6  convolutional neural network   \n",
       "\n",
       "                                            tokens_x short_form_x  \\\n",
       "0  ['If', 'we', 'compare', 'the', 'computational'...          DBN   \n",
       "1  ['Furthermore', ',', 'since', 'the', 'LSTMs', ...           SL   \n",
       "2  ['ResultsResults', 'for', 'CNN', 'trained', 'o...          CNN   \n",
       "3  ['d', ')', ':', 'LayAR', 'AR', 'browser', '.',...           AR   \n",
       "4  ['This', 'paper', 'demonstrates', 'the', 'effe...          CNN   \n",
       "\n",
       "                                            tokens_y short_form_y  pair  \n",
       "0  ['Moreover', ',', 'it', 'has', 'not', 'been', ...          DBN     1  \n",
       "1  ['Consequently', ',', 'for', 'any', 'SL', '(',...           SL     1  \n",
       "2  ['These', 'baseline', 'models', 'are', 'select...          CNN     1  \n",
       "3  ['At', 'this', 'stage', 'the', 'system', 'was'...           AR     1  \n",
       "4  ['For', 'every', 'testing', 'image', '(', 'the...          CNN     1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdc02134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 164, 7282, 1290, 1103, 149, 9272, 25866, 1132, 8362, 9823, 21601, 1118, 1103, 8148, 1996, 3776, 2463, 1195, 1336, 5363, 1115, 1103, 149, 9272, 2107, 2099, 1113, 1103, 16625, 7857, 1106, 1129, 12763, 1106, 1103, 3200, 1113, 1103, 27103, 7857, 119, 166, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# The original data set came with standard tokenized data. \n",
    "# However, BERT Models have their own spefic rules for tokenization, which means that we first have to 'untokenize'\n",
    "# the sentences.\n",
    "\n",
    "# function to untokenize the sdu sentences\n",
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    text = ''.join(words)\n",
    "    step1 = text.replace(',', '').replace(\"'\", \"\")\n",
    "    step2 = step1.replace('\"', \"\")\n",
    "                                                             \n",
    "    return step2\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "encoded_input = tokenizer(untokenize(data.iloc[1,4]))\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68a0ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tokens_x'] = data.apply(lambda row: tokenizer(untokenize(row['tokens_x'])),axis=1)\n",
    "data['clean_tokens_y'] = data.apply(lambda row: tokenizer(untokenize(row['tokens_y'])),axis=1)\n",
    "# data['clean_tokens_x'] = data.apply(lambda row: print(tokenizer(untokenize(row['tokens_x']))),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eae03609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cfdd312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8720139b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_metric\n\u001b[1;32m      3\u001b[0m metric \u001b[38;5;241m=\u001b[39m load_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2aead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e161b288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>acronym</th>\n",
       "      <th>expansion</th>\n",
       "      <th>tokens_x</th>\n",
       "      <th>short_form_x</th>\n",
       "      <th>tokens_y</th>\n",
       "      <th>short_form_y</th>\n",
       "      <th>pair</th>\n",
       "      <th>clean_tokens_x</th>\n",
       "      <th>clean_tokens_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12863349</td>\n",
       "      <td>19</td>\n",
       "      <td>deep belief network</td>\n",
       "      <td>['If', 'we', 'compare', 'the', 'computational'...</td>\n",
       "      <td>DBN</td>\n",
       "      <td>['Moreover', ',', 'it', 'has', 'not', 'been', ...</td>\n",
       "      <td>DBN</td>\n",
       "      <td>1</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15091181</td>\n",
       "      <td>37</td>\n",
       "      <td>strictly local</td>\n",
       "      <td>['Furthermore', ',', 'since', 'the', 'LSTMs', ...</td>\n",
       "      <td>SL</td>\n",
       "      <td>['Consequently', ',', 'for', 'any', 'SL', '(',...</td>\n",
       "      <td>SL</td>\n",
       "      <td>1</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2157740</td>\n",
       "      <td>2</td>\n",
       "      <td>convolutional neural network</td>\n",
       "      <td>['ResultsResults', 'for', 'CNN', 'trained', 'o...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>['These', 'baseline', 'models', 'are', 'select...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>1</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>13210757</td>\n",
       "      <td>4</td>\n",
       "      <td>augmented reality</td>\n",
       "      <td>['d', ')', ':', 'LayAR', 'AR', 'browser', '.',...</td>\n",
       "      <td>AR</td>\n",
       "      <td>['At', 'this', 'stage', 'the', 'system', 'was'...</td>\n",
       "      <td>AR</td>\n",
       "      <td>1</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1839840</td>\n",
       "      <td>6</td>\n",
       "      <td>convolutional neural network</td>\n",
       "      <td>['This', 'paper', 'demonstrates', 'the', 'effe...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>['For', 'every', 'testing', 'image', '(', 'the...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>1</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>145</td>\n",
       "      <td>4008135</td>\n",
       "      <td>8</td>\n",
       "      <td>convolutional neural network</td>\n",
       "      <td>['The', 'architecture', 'combines', 'mainly', ...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>['CNN', 'and', 'ResNetsec', ':', 'CNNandResNet...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>1</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>146</td>\n",
       "      <td>6827419</td>\n",
       "      <td>25</td>\n",
       "      <td>computed tomography</td>\n",
       "      <td>['J.', 'Wang', ',', 'J.', 'H.', 'Noble', ',', ...</td>\n",
       "      <td>CT</td>\n",
       "      <td>['LIDC', '-', 'LDRI', 'is', 'a', 'large', '-',...</td>\n",
       "      <td>CT</td>\n",
       "      <td>1</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>147</td>\n",
       "      <td>8506217</td>\n",
       "      <td>30</td>\n",
       "      <td>forward error correction</td>\n",
       "      <td>['However', ',', 'due', 'to', 'the', 'video', ...</td>\n",
       "      <td>FEC</td>\n",
       "      <td>['In', 'order', 'to', 'adjust', 'the', 'FEC', ...</td>\n",
       "      <td>FEC</td>\n",
       "      <td>1</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>148</td>\n",
       "      <td>14213057</td>\n",
       "      <td>40</td>\n",
       "      <td>random neural networks</td>\n",
       "      <td>['At', 'lines', '04', ',', '05', ',', '06', ',...</td>\n",
       "      <td>RNN</td>\n",
       "      <td>['After', 'that', ',', 'the', 'RNN', 'can', 'b...</td>\n",
       "      <td>RNN</td>\n",
       "      <td>1</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>149</td>\n",
       "      <td>3872823</td>\n",
       "      <td>8</td>\n",
       "      <td>convolutional neural network</td>\n",
       "      <td>['applied', 'a', 'gradient', '-', 'based', 'le...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>['In', 'this', 'paper', ',', 'we', 'make', 'in...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>1</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     index  acronym                     expansion  \\\n",
       "0             0  12863349       19           deep belief network   \n",
       "1             1  15091181       37                strictly local   \n",
       "2             2   2157740        2  convolutional neural network   \n",
       "3             3  13210757        4             augmented reality   \n",
       "4             4   1839840        6  convolutional neural network   \n",
       "..          ...       ...      ...                           ...   \n",
       "145         145   4008135        8  convolutional neural network   \n",
       "146         146   6827419       25           computed tomography   \n",
       "147         147   8506217       30      forward error correction   \n",
       "148         148  14213057       40        random neural networks   \n",
       "149         149   3872823        8  convolutional neural network   \n",
       "\n",
       "                                              tokens_x short_form_x  \\\n",
       "0    ['If', 'we', 'compare', 'the', 'computational'...          DBN   \n",
       "1    ['Furthermore', ',', 'since', 'the', 'LSTMs', ...           SL   \n",
       "2    ['ResultsResults', 'for', 'CNN', 'trained', 'o...          CNN   \n",
       "3    ['d', ')', ':', 'LayAR', 'AR', 'browser', '.',...           AR   \n",
       "4    ['This', 'paper', 'demonstrates', 'the', 'effe...          CNN   \n",
       "..                                                 ...          ...   \n",
       "145  ['The', 'architecture', 'combines', 'mainly', ...          CNN   \n",
       "146  ['J.', 'Wang', ',', 'J.', 'H.', 'Noble', ',', ...           CT   \n",
       "147  ['However', ',', 'due', 'to', 'the', 'video', ...          FEC   \n",
       "148  ['At', 'lines', '04', ',', '05', ',', '06', ',...          RNN   \n",
       "149  ['applied', 'a', 'gradient', '-', 'based', 'le...          CNN   \n",
       "\n",
       "                                              tokens_y short_form_y  pair  \\\n",
       "0    ['Moreover', ',', 'it', 'has', 'not', 'been', ...          DBN     1   \n",
       "1    ['Consequently', ',', 'for', 'any', 'SL', '(',...           SL     1   \n",
       "2    ['These', 'baseline', 'models', 'are', 'select...          CNN     1   \n",
       "3    ['At', 'this', 'stage', 'the', 'system', 'was'...           AR     1   \n",
       "4    ['For', 'every', 'testing', 'image', '(', 'the...          CNN     1   \n",
       "..                                                 ...          ...   ...   \n",
       "145  ['CNN', 'and', 'ResNetsec', ':', 'CNNandResNet...          CNN     1   \n",
       "146  ['LIDC', '-', 'LDRI', 'is', 'a', 'large', '-',...           CT     1   \n",
       "147  ['In', 'order', 'to', 'adjust', 'the', 'FEC', ...          FEC     1   \n",
       "148  ['After', 'that', ',', 'the', 'RNN', 'can', 'b...          RNN     1   \n",
       "149  ['In', 'this', 'paper', ',', 'we', 'make', 'in...          CNN     1   \n",
       "\n",
       "                                  clean_tokens_x  \\\n",
       "0    [input_ids, token_type_ids, attention_mask]   \n",
       "1    [input_ids, token_type_ids, attention_mask]   \n",
       "2    [input_ids, token_type_ids, attention_mask]   \n",
       "3    [input_ids, token_type_ids, attention_mask]   \n",
       "4    [input_ids, token_type_ids, attention_mask]   \n",
       "..                                           ...   \n",
       "145  [input_ids, token_type_ids, attention_mask]   \n",
       "146  [input_ids, token_type_ids, attention_mask]   \n",
       "147  [input_ids, token_type_ids, attention_mask]   \n",
       "148  [input_ids, token_type_ids, attention_mask]   \n",
       "149  [input_ids, token_type_ids, attention_mask]   \n",
       "\n",
       "                                  clean_tokens_y  \n",
       "0    [input_ids, token_type_ids, attention_mask]  \n",
       "1    [input_ids, token_type_ids, attention_mask]  \n",
       "2    [input_ids, token_type_ids, attention_mask]  \n",
       "3    [input_ids, token_type_ids, attention_mask]  \n",
       "4    [input_ids, token_type_ids, attention_mask]  \n",
       "..                                           ...  \n",
       "145  [input_ids, token_type_ids, attention_mask]  \n",
       "146  [input_ids, token_type_ids, attention_mask]  \n",
       "147  [input_ids, token_type_ids, attention_mask]  \n",
       "148  [input_ids, token_type_ids, attention_mask]  \n",
       "149  [input_ids, token_type_ids, attention_mask]  \n",
       "\n",
       "[150 rows x 11 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c28b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457ff76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d33e579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.2.1-py3-none-any.whl (342 kB)\n",
      "\u001b[K     |████████████████████████████████| 342 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pandas in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: aiohttp in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: packaging in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from datasets) (4.63.0)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.21.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-macosx_10_9_x86_64.whl (34 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyyaml in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jesher/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: dill, xxhash, responses, multiprocess, datasets\n",
      "Successfully installed datasets-2.2.1 dill-0.3.4 multiprocess-0.70.12.2 responses-0.18.0 xxhash-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb87400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
